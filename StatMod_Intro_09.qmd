---
subtitle: "Neunter Termin"
---

# Einstieg

## Zur Erinnerung

-   :computer: Arbeiten Sie aktiv mit.

-   :raising_hand: Stellen Sie Fragen.

-   :muscle: <https://tweedback.de/xxx/>

::: center
{{< qrcode https://tweedback.de/xxx/quiz width=400 height=400 >}}
:::

## Tipps f√ºr den Vorlesungserfolg

-   Kommen Sie zur Vorlesung!

-   Vermeiden Sie Ablenkung.

-   Arbeiten Sie die Vorlesung von Anfang an **vor** und nach. Nutzen Sie daf√ºr das Dokument *Quantitative Datenanalyse -- Umsetzung mit R*.

-   Stellen Sie Fragen.

-   Unterst√ºtzen Sie sich gegenseitig.

# R√ºckblick

## Beim letzten Mal haben Sie gelernt, dass ...

-   Random Forest eine Ensemble-Methode ist, die mehrere Entscheidungsb√§ume zu einem Vorhersagemodell aggregiert.
-   Random Forest sowohl f√ºr Klassifikations- als auch Regressionsprobleme verwendet werden kann.
-   Random Forest "Bagging" verwendet, um die einzelnen Entscheidungsb√§ume zu diversifizieren und somit die Varianz des Vorhersagemodells zu reduzieren.
-   zum Trainieren und Testen des Algorithmus der urspr√ºngliche Datensatz in Trainings- und Testdatensatz aufgeteilt werden muss.
-   beim Trainieren des Algorithmus Hyperparameter eingestellt werden k√∂nnen, wie z. B. `ntree` oder `mtry`.
-   die Performance des Algorithmus mithilfe von Modellg√ºtekriterien √ºberpr√ºft werden kann.
-   nachvollzogen werden kann, welchen Einfluss die verschiedenen Variablen auf die Vorhersage haben.

## Heutige Themen -- Unsupervised Learning üè´

**Un√ºberwachtes Lernen** (engl.: unsupervised learning): Es gibt *keine* bekannte abh√§ngige Variable $y$, die modelliert werden soll.

:::::: columns
::: {.column width="50%"}
Methoden (u. a.):

-   Clusteranalyse

-   Hauptkomponentenanalyse
:::

:::: {.column width="50%"}
::: center
![](img/Memes/Meme_PCA-CA.jpg){width="80%"}
:::
::::
::::::

# Hauptkomponentenanalyse (PCA)

## Was Sie lernen üë©‚Äçüè´

-   Sie wissen, wann eine Dimensionsreduktion von Daten sinnvoll sein kann.

-   Sie k√∂nnen die Voraussetzungen f√ºr eine Hauptkomponentenanalyse √ºberpr√ºfen\
    und die Analyse durchf√ºhren.

-   Sie wissen, warum eine Rotation zum Einsatz kommen kann.

-   Sie k√∂nnen die Ergebnisse einer Hauptkomponentenanalyse interpretieren.

## Steckbrief Hauptkomponentenanalyse

-   Die Hauptkomponentenanalyse (Principal Component Analysis ‚Äì PCA) ist eine statistische Methode zur Dimensionsreduktion.

-   **Idee**: Fasse korrelierte Variablen (linear) zusammen. Die resultierenden Komponenten sind unkorreliert und beinhalten einen m√∂glichst gro√üen Anteil der (multivariaten) Gesamtvariation.

-   **Frage**: K√∂nnen multidimensionale metrische Daten auf wenige wichtige Hauptkomponenten (Faktoren) verdichtet werden?

-   Das **Ziel** der Analyse bestehet darin, aus einer Vielzahl empirisch beobachteter, korrelierter Variablen (z.¬†B. Fragebogenitems) auf wenige voneinander unabh√§ngige Variablen (Hauptkomponenten) zu schlie√üen (Dimensionsreduktion).

## Einstiegsbeispiel Hauptkomponentenanalyse

Im Rahmen einer Studie zu Marktreaktionsfunktionen wurde - neben vielen anderen Variablen - der Erfolg eines Unternehmens √ºber folgende Items erhoben:

::: center
![Fragebogen-Auszug](img/Sonstige/Unternehmenserfolg-FB.png){width="50%"}
:::

Frage: Gibt es (weniger als 6) Hauptkomponenten, die den Erfolg des Unternehmens erkl√§ren?

::: footnote
[Gansser, O.A. & Krol, B. (2015)](https://link.springer.com/chapter/10.1007/978-3-658-04492-3_8): Ein Modell zur Erkl√§rung und Prognose des Werbeplanungserfolgs.
:::

## Ausgangsbasis: Korrelationsmatrix

Darstellung der bivariaten Korrelation der sechs metrischen Variablen:

<br>

```{r}
#| label: Korrelationsmatrix

# Paket laden
library(psych)

# Daten einlesen
Erfolg <- read.csv2("https://raw.githubusercontent.com/FOM-ifes/variousdata/refs/heads/main/Erfolg.csv")

# Korrelationsmatrix
cormatrix <- round(cor(Erfolg),3)

library(knitr)
library(kableExtra)
kable(cormatrix, format = "html") |>
  kable_styling(font_size = 36)

```

## Corrplot

Um einen besseren √úberblick √ºber die Korrelationen zu bekommen, kann eine grafische Analyse mittels `corrplot` durchgef√ºhrt werden:

```{r}
#| label: Corrplot
#| fig-height: 3.5

# Paket laden
library(corrplot)

# Corrplot
corrplot(cor(Erfolg))
```

## Vier Schritte der PCA

1.  Voraussetzungen, d.¬†h. Eignung der Korrelationsmatrix pr√ºfen

2.  Anzahl der Hauptkomponenten bestimmen

3.  Urspr√ºngliche Variablen den Hauptkomponenten zuordnen

4.  ggf. Reliabilit√§tsanalyse durchf√ºhren

## Voraussetzungen der PCA

[Korrelationsanalyse]{.green}

::: {style="font-size: 97%;"}
-   zeigt die paarweisen Korrelationen der Variablen an
-   gibt erste Hinweis auf Zusammenh√§nge, ist aber als Pr√ºfung der Voraussetzungen nicht ausreichend
:::

[Kaiser-Mayer-Olkin-Kriterium (KMO)]{.green}

::: {style="font-size: 97%;"}
-   zeigt an, inwieweit die Ausgangsdaten miteinander im Zusammenhang stehen.
-   KMO \< 0,5 ist nicht akzeptabel, w√ºnschenswert ist KMO \> 0,8.
:::

[Bartlett-Test]{.green}

::: {style="font-size: 80%;"}
-   pr√ºft mittels $\chi^2$-Test, ob die Korrelationsmatrix sich von der Einheitsmatrix unterscheidet:

    + Nullhypothese ($H_0$): Korrelationsmatrix entspricht der Einheitsmatrix.
    + Alternativhypothese ($H_1$): Korrelationsmatrix ist ungleich der Einheitsmatrix.

:::

## Anzahl der Hauptkomponenten: Vorgehen

Wie erh√§lt man die Hauptkomponenten und wie viele sind sinnvoll?

-   Zun√§chst gibt es so viele Hauptkomponenten wie Variablen

-   Ziel: Dimensionsreduktion durch max. Erfassung der Variation und [sinnvoll interpretierbare]{.green} Hauptkomponenten

-   Schritt 1: Suche die gr√∂√üte Gruppe von Items, die hoch korreliert sind (1. Hauptkomponente)

-   Schritt 2: Suche die zweitgr√∂√üte Gruppe von Items, die hoch korreliert sind (2. Hauptkomponente)

-   Wiederholung der Schritte 1 und 2, bis die Anzahl der Hauptkomponenten der Anzahl von Items entspricht

-   Beachtung von ‚ÄûStopp-Regeln‚Äú, um wichtige von unwichtigen Hauptkomponenten zu trennen (s.¬†n.¬†S.)

## Anzahl der Hauptkomponenten: Stopp-Kriterien

::: {style="font-size: 90%;"}
[Eigenwert als Ausgangsbasis]{.green}
:::

::: {style="font-size: 87%;"}
-   Der Eigenwert einer Hauptkomponente entspricht der von ihr bei allen Variablen gemeinsam erkl√§rten Streuung.
-   Konstruktionsbedingt ist dabei der Eigenwert der ersten HKA der h√∂chste, der Eigenwert der zweiten HKA der zweith√∂chste usw.
:::

::: {style="font-size: 90%;"}
[Kaiser-Kriterium]{.green}
:::

::: {style="font-size: 87%;"}
-   Ber√ºcksichtigung der Hauptkomponenten mit einem Eigenwert \> 1
-   Grund: Hauptkomponenten mit einem Eigenwert \<1 haben weniger Erkl√§rungswert als urspr√ºngliche Variablen
:::

::: {style="font-size: 90%;"}
[Scree-Plot-Kriterium]{.green} (Ellenbogenkriterium)
:::

::: {style="font-size: 87%;"}
-   Graphisches Verfahren zur Bestimmung der optimalen Faktorenzahl: Ber√ºcksichtigung aller Hauptkomponenten, die im Scree-Plot links der Knickstelle (des Ellenbogens) liegen.
-   Gibt es mehrere Knicke, dann w√§hlt man jene Hauptkomponente, die links vor dem rechtesten Knick liegen.
:::


## Scree-Plot

::: center
```{r}
#| label: screeplot
#| fig-width: 15
#| fig-height: 8

scree(Erfolg, factors = FALSE)
```
:::

## Durchf√ºhrung der PCA: Extraktion der Komponenten

```{r}
#| label: PCA

Erfolg.pca <- principal(Erfolg, nfactors = 2, rotate = "none")

```

![](img/Sonstige/Print-Erfolg-PCA.png){fig-align="center" height="22cm"}


## Durchf√ºhrung der PCA: Zuordnung der Variablen zu den Komponenten

-   Die [Faktorladung]{.green} (factor loading) bezeichnet die Korrelation einer Variablen mit einer Hauptkomponente.\
    Ist die Faktorladung [hoch]{.green}, dann beschreibt die Hauptkomponente die Variable [gut]{.green}, andernfalls nicht. Dementsprechend wird eine Variable einer Hauptkomponente zugeordnet, wenn die Faktorladung wenigstens 0,4 betr√§gt.

-   Die [durchschnittlich erfasste Varianz]{.green} gibt an, wie viel Prozent der Ursprungsvarianz (Ursprungsinformation) der Ausgangsvariablen durch die Gesamtzahl aller Hauptkomponenten erfasst wird. Sie sollte mindestens $50\,\%$ betragen.

## Durchf√ºhrung der PCA: Komponentenladungen

-   Hauptkomponenten (Spalten) sind Linearkombinationen der Items (Zeilen)

-   Die Korrelation zwischen Item und Hauptkomponente wird als Komponentenladung (kurz: Ladung) bezeichnet.

-   Items, die hohe Ladungen zu einer Komponente aufweisen, werden zu dieser [verdichtet]{.green} und man versucht, einen [Namen]{.green} f√ºr die gemeinsamen Eigenschaften dieser Items zu finden.

::::: columns
::: {.column width="50%"}
Ergebnis des Beispiels *ohne Rotation*:
:::

::: {.column width="50%"}
```{r}
#| label: PCA-loadings

pcaloadings <- round(Erfolg.pca$loadings[,], 2)

kable(pcaloadings, format = "html") |>
  kable_styling(font_size = 36)

```
:::
:::::

::: footnote
Variablen: 1: Cash Flow, 2: Umsatz-Rentabilit√§t, 3: Absatzwachstum, 4: Marktanteile,5: Kundenzufriedenheit, 6: Kundenbindung 
::: 


## Durchf√ºhrung der PCA: Rotation (I / II)

unrotierte Komponentenladungen

```{r}
#| label: Plot-unrotiert
#| fig-height: 4

# Pakete laden
library(mosaic)
library(ggplot2)
library(ggformula)
library(ggrepel)  # Falls du √ºberlappende Labels vermeiden willst

df_loadings <- as.data.frame(Erfolg.pca$loadings[, 1:2]) %>%
  mutate(Variable = rownames(Erfolg.pca$loadings)) %>%
  mutate(Variable = gsub("q_012_SQ00", "", Variable))  # Ersetze Muster durch K√ºrzel

ggplot(df_loadings, aes(x = PC1, y = PC2, label = Variable)) +
  geom_point(size=3.5) +
  geom_text_repel(size = 4) +  # Verhindert √úberlappungen der Bezeichnungen
 geom_hline(yintercept = 0, color = "black", arrow = arrow(length = unit(1, "cm"))) +  # Horizontale Achse
 geom_vline(xintercept = 0, color = "black", arrow = arrow(length = unit(0.15, "cm"))) +  # Vertikale Achse
  xlim(-1, 1.15) +  # Setzt die x-Achsen-Grenzen
  ylim(-1, 1) +  # Setzt die y-Achsen-Grenzen
  xlab("Hauptkomponente 1 (PC1)") +
  ylab("Hauptkomponente 2 (PC2)") +
  ggtitle("PCA Ladungen") +
  theme_minimal() +
  coord_fixed() +  # Gleiche Skalierung der Achsen
  theme(axis.text = element_text(size = 12),
        axis.title = element_text(size = 14, face = "bold"),
        plot.title = element_text(size = 16, face = "bold"))
```

## Durchf√ºhrung der PCA: Rotation (II / II)

::::: columns
::: {.column width="50%"}
```{r}
#| label: Plot-vor-Rotation
#| fig-height: 9.5

ggplot(df_loadings, aes(x = PC1, y = PC2, label = Variable)) +
  geom_point(size=3.5) +
  geom_text_repel(size = 6) +  # Verhindert √úberlappungen der Bezeichnungen
  geom_hline(yintercept = 0, color = "black") +  # Horizontale Achse
  geom_vline(xintercept = 0, color = "black") +  # Vertikale Achse
  xlim(-1, 1.15) +  # Setzt die x-Achsen-Grenzen
  ylim(-1, 1) +  # Setzt die y-Achsen-Grenzen
# 45-Grad- und -45-Grad-Achsen
  geom_abline(slope = 1, intercept = 0, linetype = "dashed", color = "red") +  # 45¬∞
  geom_abline(slope = -1, intercept = 0, linetype = "dashed", color = "blue") + # -45¬∞
# Pfeile zur Illustration der Rotation
  geom_curve(aes(x = 0.0, y = 0.6, xend = 0.4, yend = 0.4), 
             arrow = arrow(length = unit(0.15, "inches")), curvature = -0.3, color = "red") + # Pfeil zur neuen Achse
  geom_curve(aes(x = 0.6, y = 0.0, xend = 0.4, yend = -0.4), 
             arrow = arrow(length = unit(0.15, "inches")), curvature = -0.3, color = "blue") + # Pfeil zur neuen Achse
  xlab("Hauptkomponente 1 (PC1)") +
  ylab("Hauptkomponente 2 (PC2)") +
  ggtitle("unrotierte Achsen") +
  theme_minimal() +
  coord_fixed() +  # Gleiche Skalierung der Achsen
  theme(axis.text = element_text(size = 16),
        axis.title = element_text(size = 20, face = "bold"),
        plot.title = element_text(size = 28, face = "bold"))
```
:::

::: {.column width="50%"}
```{r}
#| label: Plot--nach-Rotation
#| fig-height: 9.5

# PCA rotiert
Erfolg.pcarot <- principal(Erfolg, nfactors = 2)

# Visualisierung

df_loadingsrot <- as.data.frame(Erfolg.pcarot$loadings[, 1:2]) %>%
  mutate(Variable = rownames(Erfolg.pcarot$loadings)) %>%
  mutate(Variable = gsub("q_012_SQ00", "", Variable))  # Ersetze Muster durch K√ºrzel

ggplot(df_loadingsrot, aes(x = RC2, y = RC1, label = Variable)) +
  geom_point(size=3.5) +
  geom_text_repel(size = 6) +  # Verhindert √úberlappungen der Bezeichnungen
  geom_hline(yintercept = 0,  color = "blue") +  # Horizontale Achse RC1
  geom_vline(xintercept = 0, color = "red") +  # Vertikale Achse RC2
  xlim(-1, 1.15) +  # Setzt die x-Achsen-Grenzen
  ylim(-1, 1) +  # Setzt die y-Achsen-Grenzen
  xlab("rotierte Hauptkomponente 2") +
  ylab("rotierte Hauptkomponente 1 ") +
  ggtitle("rotierte Achsen") +
  theme_minimal() +
  coord_fixed() +  # Gleiche Skalierung der Achsen
  theme(axis.text = element_text(size = 16),
        axis.title = element_text(size = 20, face = "bold"),
        plot.title = element_text(size = 28, face = "bold"))	

```
:::
:::::

## Rotationsverfahren

Die Rotation soll die Hauptkomponenten inhaltlich besser interpretierbar machen. Zur Verf√ºgung stehen verschiedene Verfahren, darunter:

-   [orthogonale]{.green} Verfahren: die rotierten Faktoren sind unkorreliert (z.¬†B. [Varimax]{.green})

-   [schiefwinklige]{.green} Verfahren: die rotierten Faktoren sind korreliert (z.¬†B. [Promax]{.green})


## Durchf√ºhrung der PCA mit Varimax-Rotation

![](img/Sonstige/Print-Erfolg-PCArot.png){fig-align="center" height="22cm"}


## Grafische Darstellung

:::: {.columns}

::: {.column width="50%"}

```{r}
#| label: fa.diagram
#| echo: true
#| eval: false

fa.diagram(Erfolg.pcarot)
```

![](img/Sonstige/fa-diagram.png){fig-align="center" height="15cm"}

:::

:::{.column width="50%"}

Antwort auf die auf S. 11 gestellte Frage:

> Der mehrdimensional gemessene Unternehmenserfolg kann durch zwei Hauptkomponenten repr√§sentiert werden (kumulierte Varianz = 0,78).\
> \
> Die erste Hauptkomponente umfasst den √∂konomischen Erfolg, die zweite Hauptkomponente den vor√∂konomische Erfolg.

:::

::::

## Verwendung der Ergebnisse f√ºr weitere Analysen

-  Das mittels Hauptkomponentenanalyse (ggf.) aufgedeckte Reduktionspotenzial in den Daten kann in weiter Analysen einflie√üen.

-  Um mit den Hauptkomponenten als neue Variablen weiter arbeiten zu k√∂nnen, m√ºssen diese berechnet werden.

-  Dazu stehen verschiedene Methoden zur Verf√ºgung, u.¬†a.

    - Mittelwertindex aus den zu einer Hauptkomponente verdichteten Items
    
    - Verwendung der Ladungen
    
    
## Fallstudie üíª

::::::: columns
:::: {.column width="50%"}
-   posit Cloud: In **Ihr** Projekt einloggen.

::: center
![](img/Software/posit_Project_StatistischeModellierung.png){width="80%"}
:::
::::

:::: {.column width="50%"}
-   Lokal: RStudio durch Klick auf `StatMod_WiSe24.Rproj` starten oder RStudio aufrufen, das letzte Projekt m√ºsste automatisch geladen werden.

::: center
![](img/Software/RStudio_Project_StatistischeModellierung.png){width="60%"}
:::
::::
:::::::

√ñffnen Sie die Datei `HKA.qmd` im Ordner `fallstudien`.

# Clusteranalyse

## Was Sie lernen üë©‚Äçüè´

-   Sie k√∂nnen das Ziel einer Clusteranalyse erl√§utern.

-   Sie kennen das Konzept der euklidischen Distanz.

-   Sie k√∂nnen eine k-Means-Clusteranalyse durchf√ºhren und die Ergebnisse interpretieren.

## Was ist eine Clusteranalyse?

Das Ziel der Clusteranalyse (engl.: Cluster Analysis) ist es, Gruppen (Cluster) von Beobachtungen zu bilden, die innerhalb der Cluster homogen und zwischen den Clustern heterogen sind. 

Einsatzbeispiel sind:

-   Analysieren, inwieweit es unterschiedliche Gruppen von Unternehmen am Markt gibt.

-   Finden von Kundensegmenten auf Basis von Kundendaten.

-   Identifizieren von Clustern (Typen) verschiedener F√ºhrungspers√∂nlichkeiten.

-   Auch die Gesichtserkennung auf Fotos oder Videos basiert auf dem Ansatz der Clusteranalyse.

## Methoden der Clusteranalyse

**Hierarchische Verfahren**: sukzessive Zusammenfassung der Beobachtungen

- bottom-up (agglomerativ: Start mit sovielen Clustern, wie es Beobachtungen gibt) oder

- top-down (divisiv: Start mit einem Cluster)


**Partionierende Verfahren** (Clusterzentralanalyse): Vorgabe der Anzahl von Clustern, dann bestm√∂gliche Zuordnung der Beobachtungen √ºber mehrere Schritte zu den Clustern (z. B. k-Means-Clustering)

<br>

<br>

<br>

<br>

<br>

:::footnote
Eine √úbersicht √ºber die Methoden, um $k$ (oft: $k \in \{2, 3, . . . , 10\}$, aber √ºblicherweise ist die Anzahl $k$ unbekannt!) Cluster zu finden, zeigen z. B. Charrad M., Ghazzali N., Boiteau V. and Niknafs, A. (2014). NbClust: An R Package for Determining the Relevant Number of Clusters in a Data Set. Journal of Statistical Software, 61(6), 1-36. <http://dx.doi.org/10.18637/jss.v061.i06>
:::


## Hierarchische Clusteranalyse

**Grundidee**:

- *Schritt 1*: jede Beobachtung ist ein Cluster, Berechnung Diatanzen zwischen den einzelnen Clustern 
  + Auswahl der [Distanzfunktion]{.green}
  
- *Schritt 2*: Verschmelzung jener zwei Cluster, die sich am n√§chsten sind

- *Schritt3*: Berechnung der Distanzen von neu gebildeten Cluster zu den anderen Clustern
  + verschiedene [Clustermethoden]{.green}
  
- Wiederhole die Schritte 2 und 3 so lange, bis nur noch ein Cluster, in dem alle Beobachtungen enthalten sind, vorhanden ist.


## Distanzma√ü

Verwendung von Distanzma√üen zur Bestimmung der √Ñhnlichkeit (Un√§hnlichkeit) innerhalb der Cluster. F√ºr metrische Variablen z. B. euklidische Distanz:

::::: columns
::: {.column width="50%"}

<br>

$$
d(x,y) = \sqrt{\sum_j(x_j - y_j)^2}
$$
:::


::: {.column width="50%"}
:::center
![](img/Clusteranalyse/EukDistanz.png){width="75%"}
:::
:::
:::::



## Clustermethoden (I/II)

:::center
![](img/Clusteranalyse/Clustermethoden1.png){width="45%"}
:::

## Clustermethoden (II/II)

<br>

:::center
![](img/Clusteranalyse/Clustermethoden2.png){width="45%"}
:::



## Partionierende Clusteranalyse

**k-Means Schema**:

- [*Schritt 1*]{.green}: Zuf√§llige Auswahl von $k$ Clusterzentren aus den $n$ Beobachtungen
- [*Schritt 2*]{.green}: Zuordnung der Beobachtungen zum n√§chsten Cluster
- [*Schritt 3*]{.green}: Neuberechnung der Clusterzentren als Mittelwert der dem Cluster zugeordneten Beobachtungen

<br>

- Wiederhole ie Schritte 2 und 3 so lange bis sich keine √Ñnderung der Zuordnung mehr ergibt -- oder eine maximale Anzahl der Iterationen erreicht ist.

<br>

[Nachfolgende Animation einer Clusterzentrenanalyse mit 6 Clustern stammt aus dem Vortrag zur Diplomarbeit von Dipl.-Stat. Nils Raabe.]{.green}

## Clusteranalyse

:::::{.columns}
:::{.column width="40%"}

<br>

> 1 - Auswahl von c Clusterseeds\
> \
> \
> 2 - Zuordnung aller Objekte\
> \
> \
> 3 - Ersetzen der Seeds\
> \
> \
> 4 - Abschlie√üende Zuornung aller\
> Objekte

:::

:::{.column width="60%"}

:::center
![](img/Clusteranalyse/Animation1.png){width="60%"}
:::
:::
:::::


## Clusteranalyse

:::::{.columns}
:::{.column width="40%"}

<br>

> **1 - Auswahl von c Clusterseeds**\
> \
> \
> 2 - Zuordnung aller Objekte\
> \
> \
> 3 - Ersetzen der Seeds\
> \
> \
> 4 - Abschlie√üende Zuornung aller\
> Objekte

:::

:::{.column width="60%"}

:::center
![](img/Clusteranalyse/Animation2.png){width="60%"}
:::
:::
:::::



## Clusteranalyse

:::::{.columns}
:::{.column width="40%"}

<br>

> 1 - Auswahl von c Clusterseeds\
> \
> \
> **2 - Zuordnung aller Objekte**\
> \
> \
> 3 - Ersetzen der Seeds\
> \
> \
> 4 - Abschlie√üende Zuornung aller\
> Objekte

:::

:::{.column width="60%"}

:::center
![](img/Clusteranalyse/Animation3.png){width="60%"}
:::
:::
:::::


## Clusteranalyse

:::::{.columns}
:::{.column width="40%"}

<br>

> 1 - Auswahl von c Clusterseeds\
> \
> \
> 2 - Zuordnung aller Objekte\
> \
> \
> **3 - Ersetzen der Seeds**\
> \
> \
> 4 - Abschlie√üende Zuornung aller\
> Objekte

:::

:::{.column width="60%"}

:::center
![](img/Clusteranalyse/Animation4.png){width="60%"}
:::
:::
:::::


## Clusteranalyse

:::::{.columns}
:::{.column width="40%"}

<br>

> 1 - Auswahl von c Clusterseeds\
> \
> \
> **2 - Zuordnung aller Objekte**\
> \
> \
> 3 - Ersetzen der Seeds\
> \
> \
> 4 - Abschlie√üende Zuornung aller\
> Objekte

:::

:::{.column width="60%"}

:::center
![](img/Clusteranalyse/Animation5.png){width="60%"}
:::
:::
:::::


## Clusteranalyse

:::::{.columns}
:::{.column width="40%"}

<br>

> 1 - Auswahl von c Clusterseeds\
> \
> \
> 2 - Zuordnung aller Objekte\
> \
> \
> **3 - Ersetzen der Seeds**\
> \
> \
> 4 - Abschlie√üende Zuornung aller\
> Objekte

:::

:::{.column width="60%"}

:::center
![](img/Clusteranalyse/Animation6.png){width="60%"}
:::
:::
:::::


## Clusteranalyse

:::::{.columns}
:::{.column width="40%"}

<br>

> 1 - Auswahl von c Clusterseeds\
> \
> \
> **2 - Zuordnung aller Objekte**\
> \
> \
> 3 - Ersetzen der Seeds\
> \
> \
> 4 - Abschlie√üende Zuornung aller\
> Objekte

:::

:::{.column width="60%"}

:::center
![](img/Clusteranalyse/Animation7.png){width="60%"}
:::
:::
:::::


## Clusteranalyse

:::::{.columns}
:::{.column width="40%"}

<br>

> 1 - Auswahl von c Clusterseeds\
> \
> \
> 2 - Zuordnung aller Objekte\
> \
> \
> **3 - Ersetzen der Seeds**\
> \
> \
> 4 - Abschlie√üende Zuornung aller\
> Objekte

:::

:::{.column width="60%"}

:::center
![](img/Clusteranalyse/Animation8.png){width="60%"}
:::
:::
:::::


## Clusteranalyse

:::::{.columns}
:::{.column width="40%"}

<br>

> 1 - Auswahl von c Clusterseeds\
> \
> \
> **2 - Zuordnung aller Objekte**\
> \
> \
> 3 - Ersetzen der Seeds\
> \
> \
> 4 - Abschlie√üende Zuornung aller\
> Objekte

:::

:::{.column width="60%"}

:::center
![](img/Clusteranalyse/Animation9.png){width="60%"}
:::
:::
:::::


## Clusteranalyse

:::::{.columns}
:::{.column width="40%"}

<br>

> 1 - Auswahl von c Clusterseeds\
> \
> \
> 2 - Zuordnung aller Objekte\
> \
> \
> **3 - Ersetzen der Seeds**\
> \
> \
> 4 - Abschlie√üende Zuornung aller\
> Objekte

:::

:::{.column width="60%"}

:::center
![](img/Clusteranalyse/Animation10.png){width="60%"}
:::
:::
:::::


## Clusteranalyse

:::::{.columns}
:::{.column width="40%"}

<br>

> 1 - Auswahl von c Clusterseeds\
> \
> \
> **2 - Zuordnung aller Objekte**\
> \
> \
> 3 - Ersetzen der Seeds\
> \
> \
> 4 - Abschlie√üende Zuornung aller\
> Objekte

:::

:::{.column width="60%"}

:::center
![](img/Clusteranalyse/Animation11.png){width="60%"}
:::
:::
:::::


## Clusteranalyse

:::::{.columns}
:::{.column width="40%"}

<br>

> 1 - Auswahl von c Clusterseeds\
> \
> \
> 2 - Zuordnung aller Objekte\
> \
> \
> **3 - Ersetzen der Seeds**\
> \
> \
> 4 - Abschlie√üende Zuornung aller\
> Objekte

:::

:::{.column width="60%"}

:::center
![](img/Clusteranalyse/Animation12.png){width="60%"}
:::
:::
:::::


## Clusteranalyse

:::::{.columns}
:::{.column width="40%"}

<br>

> 1 - Auswahl von c Clusterseeds\
> \
> \
> 2 - Zuordnung aller Objekte\
> \
> \
> 3 - Ersetzen der Seeds\
> \
> \
> **4 - Abschlie√üende Zuornung aller**\
> **Objekte**

:::

:::{.column width="60%"}

:::center
![](img/Clusteranalyse/Animation13.png){width="60%"}
:::
:::
:::::



## Fallstudie üíª

::::::: columns
:::: {.column width="50%"}
-   posit Cloud: In **Ihr** Projekt einloggen.

::: center
![](img/Software/posit_Project_StatistischeModellierung.png){width="80%"}
:::
::::

:::: {.column width="50%"}
-   Lokal: RStudio durch Klick auf `StatMod_WiSe24.Rproj` starten oder RStudio aufrufen, das letzte Projekt m√ºsste automatisch geladen werden.

::: center
![](img/Software/RStudio_Project_StatistischeModellierung.png){width="60%"}
:::
::::
:::::::

√ñffnen Sie die Datei `Clusteranalyse_PENGUINS.qmd` im Ordner `fallstudien`.
